{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('pyspark-training').master('local[2]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Mohit.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-training</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ccb4f4a4c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1641094265884'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641094263342"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://Mohit.mshome.net:4040'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an rdd using range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(11,20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_range = sc.range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 13, 16, 19]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.range(10,20,3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_range.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating an rdd from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_detail = [('Adam', 26), ('Jack', 25), ('Rachel', 29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_emp = sc.parallelize(emp_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd_emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_count = rdd_emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fetch the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_first = rdd_emp.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Adam', 26)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adam'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_first[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collect the rdd  - collect should be avoided in the case of huge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Adam', 26), ('Jack', 25), ('Rachel', 29)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_emp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam-26\n",
      "Jack-25\n",
      "Rachel-29\n"
     ]
    }
   ],
   "source": [
    "for i in rdd_emp.collect():\n",
    "    print(i[0] + '-' + str(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = [i[0] for i in rdd_emp.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adam', 'Jack', 'Rachel']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Adam', 26), ('Jack', 25)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_emp.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_split(x):\n",
    "    return x.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Apache',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'a',\n",
       "  'fast',\n",
       "  'and',\n",
       "  'general-purpose',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'system.',\n",
       "  'It',\n",
       "  'provides',\n",
       "  'high-level',\n",
       "  'APIs',\n",
       "  'in',\n",
       "  'Java,',\n",
       "  'Scala,',\n",
       "  'Python',\n",
       "  'and',\n",
       "  'R,',\n",
       "  'and',\n",
       "  'an',\n",
       "  'optimized',\n",
       "  'engine',\n",
       "  'that',\n",
       "  'supports',\n",
       "  'general',\n",
       "  'execution',\n",
       "  'graphs.',\n",
       "  'It',\n",
       "  'also',\n",
       "  'supports',\n",
       "  'a',\n",
       "  'rich',\n",
       "  'set',\n",
       "  'of',\n",
       "  'higher-level',\n",
       "  'tools',\n",
       "  'including',\n",
       "  'Spark',\n",
       "  'SQL',\n",
       "  'for',\n",
       "  'SQL',\n",
       "  'and',\n",
       "  'structured',\n",
       "  'data',\n",
       "  'processing,',\n",
       "  'MLlib',\n",
       "  'for',\n",
       "  'machine',\n",
       "  'learning,',\n",
       "  'GraphX',\n",
       "  'for',\n",
       "  'graph',\n",
       "  'processing,',\n",
       "  'and',\n",
       "  'Spark',\n",
       "  'Streaming']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x : x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_rdd = rdd.map(lambda x : x.split(' ')).flatMap(lambda x : x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fast',\n",
       " 'and',\n",
       " 'general-purpose',\n",
       " 'cluster',\n",
       " 'computing',\n",
       " 'system.',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'high-level',\n",
       " 'APIs',\n",
       " 'in',\n",
       " 'Java,',\n",
       " 'Scala,',\n",
       " 'Python',\n",
       " 'and',\n",
       " 'R,',\n",
       " 'and',\n",
       " 'an',\n",
       " 'optimized',\n",
       " 'engine',\n",
       " 'that',\n",
       " 'supports',\n",
       " 'general',\n",
       " 'execution',\n",
       " 'graphs.',\n",
       " 'It',\n",
       " 'also',\n",
       " 'supports',\n",
       " 'a',\n",
       " 'rich',\n",
       " 'set',\n",
       " 'of',\n",
       " 'higher-level',\n",
       " 'tools',\n",
       " 'including',\n",
       " 'Spark',\n",
       " 'SQL',\n",
       " 'for',\n",
       " 'SQL',\n",
       " 'and',\n",
       " 'structured',\n",
       " 'data',\n",
       " 'processing,',\n",
       " 'MLlib',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning,',\n",
       " 'GraphX',\n",
       " 'for',\n",
       " 'graph',\n",
       " 'processing,',\n",
       " 'and',\n",
       " 'Spark',\n",
       " 'Streaming']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign 1 to each word and use reduceByKey to sum each occurance of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paired rdd -> rdd of key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 1),\n",
       " ('Spark', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('fast', 1),\n",
       " ('and', 1),\n",
       " ('general-purpose', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('system.', 1),\n",
       " ('It', 1),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('in', 1),\n",
       " ('Java,', 1),\n",
       " ('Scala,', 1),\n",
       " ('Python', 1),\n",
       " ('and', 1),\n",
       " ('R,', 1),\n",
       " ('and', 1),\n",
       " ('an', 1),\n",
       " ('optimized', 1),\n",
       " ('engine', 1),\n",
       " ('that', 1),\n",
       " ('supports', 1),\n",
       " ('general', 1),\n",
       " ('execution', 1),\n",
       " ('graphs.', 1),\n",
       " ('It', 1),\n",
       " ('also', 1),\n",
       " ('supports', 1),\n",
       " ('a', 1),\n",
       " ('rich', 1),\n",
       " ('set', 1),\n",
       " ('of', 1),\n",
       " ('higher-level', 1),\n",
       " ('tools', 1),\n",
       " ('including', 1),\n",
       " ('Spark', 1),\n",
       " ('SQL', 1),\n",
       " ('for', 1),\n",
       " ('SQL', 1),\n",
       " ('and', 1),\n",
       " ('structured', 1),\n",
       " ('data', 1),\n",
       " ('processing,', 1),\n",
       " ('MLlib', 1),\n",
       " ('for', 1),\n",
       " ('machine', 1),\n",
       " ('learning,', 1),\n",
       " ('GraphX', 1),\n",
       " ('for', 1),\n",
       " ('graph', 1),\n",
       " ('processing,', 1),\n",
       " ('and', 1),\n",
       " ('Spark', 1),\n",
       " ('Streaming', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_rdd.map(lambda x : (x, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 1),\n",
       " ('Spark', 3),\n",
       " ('is', 1),\n",
       " ('general-purpose', 1),\n",
       " ('It', 2),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('in', 1),\n",
       " ('Java,', 1),\n",
       " ('Scala,', 1),\n",
       " ('Python', 1),\n",
       " ('an', 1),\n",
       " ('optimized', 1),\n",
       " ('engine', 1),\n",
       " ('supports', 2),\n",
       " ('execution', 1),\n",
       " ('set', 1),\n",
       " ('of', 1),\n",
       " ('tools', 1),\n",
       " ('SQL', 2),\n",
       " ('processing,', 2),\n",
       " ('MLlib', 1),\n",
       " ('machine', 1),\n",
       " ('learning,', 1),\n",
       " ('GraphX', 1),\n",
       " ('graph', 1),\n",
       " ('a', 2),\n",
       " ('fast', 1),\n",
       " ('and', 5),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('system.', 1),\n",
       " ('R,', 1),\n",
       " ('that', 1),\n",
       " ('general', 1),\n",
       " ('graphs.', 1),\n",
       " ('also', 1),\n",
       " ('rich', 1),\n",
       " ('higher-level', 1),\n",
       " ('including', 1),\n",
       " ('for', 3),\n",
       " ('structured', 1),\n",
       " ('data', 1),\n",
       " ('Streaming', 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_rdd.map(lambda x : (x, 1)).reduceByKey(lambda x,y : x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle sofia air quality dataset - https://www.kaggle.com/hmavrodiev/sofia-air-quality-dataset\n",
    "### beta-Mercaptoethanol (bme) bme sensors for measuring pressure,temperature,humidity\n",
    "### reading a text file\n",
    "### row_id,sensor_id,location,lat,lon,timestamp,pressure,temperature,humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bme = sc.textFile('D:\\\\data\\\\sofia\\\\bme\\\\2017-07_bme280sof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd_bme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,sensor_id,location,lat,lon,timestamp,pressure,temperature,humidity',\n",
       " '1,2266,1140,42.738,23.272,2017-07-01T00:00:07,95270.27,23.46,62.48',\n",
       " '5,2292,1154,42.663000000000004,23.273000000000003,2017-07-01T00:00:08,94355.83,23.06,59.46',\n",
       " '7,3096,1558,42.7,23.36,2017-07-01T00:00:10,95155.81,26.53,44.38',\n",
       " '9,3428,1727,42.623999999999995,23.406,2017-07-01T00:00:12,94679.57,28.34,38.28']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = rdd_bme.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id,sensor_id,location,lat,lon,timestamp,pressure,temperature,humidity'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bme = rdd_bme.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,2266,1140,42.738,23.272,2017-07-01T00:00:07,95270.27,23.46,62.48',\n",
       " '5,2292,1154,42.663000000000004,23.273000000000003,2017-07-01T00:00:08,94355.83,23.06,59.46',\n",
       " '7,3096,1558,42.7,23.36,2017-07-01T00:00:10,95155.81,26.53,44.38',\n",
       " '9,3428,1727,42.623999999999995,23.406,2017-07-01T00:00:12,94679.57,28.34,38.28',\n",
       " '10,3472,1750,42.669,23.318,2017-07-01T00:00:13,94327.88,26.31,46.37']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701548"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of rows\n",
    "rdd_bme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,2266,1140,42.738,23.272,2017-07-01T00:00:07,95270.27,23.46,62.48']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the file by ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bme_split = rdd_bme.map(lambda row : row.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count the number of sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2266', '3096', '3428', '3472', '1846']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_split.map(lambda row : row[1]).distinct().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_split.map(lambda row : row[1]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_bme_split.map(lambda row : row[1]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect the sensors to a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = rdd_bme.map(lambda row : row.split(',')[1]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2266',\n",
       " '3096',\n",
       " '3428',\n",
       " '3472',\n",
       " '1846',\n",
       " '2228',\n",
       " '1954',\n",
       " '3620',\n",
       " '3436',\n",
       " '3092']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors[0:10]\n",
    "# [u'788', u'6385', u'5472', u'12077', u'10075', u'20111', u'6433', u'5564', u'6419', u'6680']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# number of records in sensor 2266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17708"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_split.filter(lambda row : row[1] == '2266').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1',\n",
       "  '2266',\n",
       "  '1140',\n",
       "  '42.738',\n",
       "  '23.272',\n",
       "  '2017-07-01T00:00:07',\n",
       "  '95270.27',\n",
       "  '23.46',\n",
       "  '62.48'],\n",
       " ['102',\n",
       "  '2266',\n",
       "  '1140',\n",
       "  '42.738',\n",
       "  '23.272',\n",
       "  '2017-07-01T00:02:33',\n",
       "  '95266.66',\n",
       "  '23.37',\n",
       "  '63.4'],\n",
       " ['201',\n",
       "  '2266',\n",
       "  '1140',\n",
       "  '42.738',\n",
       "  '23.272',\n",
       "  '2017-07-01T00:05:00',\n",
       "  '95258.14',\n",
       "  '23.37',\n",
       "  '63.22'],\n",
       " ['302',\n",
       "  '2266',\n",
       "  '1140',\n",
       "  '42.738',\n",
       "  '23.272',\n",
       "  '2017-07-01T00:07:26',\n",
       "  '95267.81',\n",
       "  '23.23',\n",
       "  '63.07'],\n",
       " ['400',\n",
       "  '2266',\n",
       "  '1140',\n",
       "  '42.738',\n",
       "  '23.272',\n",
       "  '2017-07-01T00:09:53',\n",
       "  '95267.84',\n",
       "  '23.25',\n",
       "  '63.37']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_split.filter(lambda row : row[1] == '2266').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paired rdd (rdd in key/value pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bme_paired = rdd_bme.map(lambda row : row.split(',')).map(lambda row : (row[1], row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2266',\n",
       "  ['1',\n",
       "   '2266',\n",
       "   '1140',\n",
       "   '42.738',\n",
       "   '23.272',\n",
       "   '2017-07-01T00:00:07',\n",
       "   '95270.27',\n",
       "   '23.46',\n",
       "   '62.48'])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_paired.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count the number of records for each sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1,1,1,2,3,3,4,5,6,5,4,3]\n",
    "dct = {}\n",
    "for l in lst:\n",
    "    if l in dct.keys():\n",
    "        dct[l]+=1\n",
    "    else:\n",
    "        dct[l] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3, 2: 1, 3: 3, 4: 2, 5: 2, 6: 1}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'2266': 17708,\n",
       "             '2292': 11003,\n",
       "             '3096': 17201,\n",
       "             '3428': 15258,\n",
       "             '3472': 17931,\n",
       "             '1952': 15383,\n",
       "             '1846': 18184,\n",
       "             '3512': 17070,\n",
       "             '2228': 17893,\n",
       "             '3438': 14333,\n",
       "             '1954': 18201,\n",
       "             '3620': 15696,\n",
       "             '3436': 13028,\n",
       "             '3092': 18152,\n",
       "             '2036': 17982,\n",
       "             '1962': 18216,\n",
       "             '3474': 17941,\n",
       "             '2232': 16145,\n",
       "             '2607': 18156,\n",
       "             '2224': 18025,\n",
       "             '3738': 18132,\n",
       "             '3102': 17845,\n",
       "             '2040': 9762,\n",
       "             '2216': 11661,\n",
       "             '3432': 18084,\n",
       "             '2294': 18079,\n",
       "             '2230': 17617,\n",
       "             '2264': 18208,\n",
       "             '1850': 17703,\n",
       "             '2234': 18008,\n",
       "             '3558': 6998,\n",
       "             '2262': 18184,\n",
       "             '1764': 18198,\n",
       "             '3836': 18108,\n",
       "             '2038': 18053,\n",
       "             '2323': 17370,\n",
       "             '3500': 16818,\n",
       "             '3326': 835,\n",
       "             '3328': 1238,\n",
       "             '3968': 6745,\n",
       "             '3982': 14695,\n",
       "             '3296': 12574,\n",
       "             '3642': 10198,\n",
       "             '3832': 214,\n",
       "             '4358': 7547,\n",
       "             '4467': 3965,\n",
       "             '4469': 4346,\n",
       "             '4471': 4322,\n",
       "             '4473': 4267,\n",
       "             '4475': 3957,\n",
       "             '4477': 4310,\n",
       "             '4479': 4308,\n",
       "             '4558': 3009,\n",
       "             '4608': 2022,\n",
       "             '4625': 579,\n",
       "             '4661': 83})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_paired.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 =sc.parallelize([('Phoebe', 24), ('Monica', 25), ('Rachel', 26)])\n",
    "rdd2 = sc.parallelize([('Joey', 27), ('Ross', 28), ('Chandler', 29)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_union = rdd1.union(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Phoebe', 24),\n",
       " ('Monica', 25),\n",
       " ('Rachel', 26),\n",
       " ('Joey', 27),\n",
       " ('Ross', 28),\n",
       " ('Chandler', 29)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_union.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_emp = sc.parallelize([(1, 'Williams'), (2, 'Mark')])\n",
    "rdd_dept = sc.parallelize([(1, 'HR'), (2, 'IT')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('Williams', 'HR')), (2, ('Mark', 'IT'))]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_emp.join(rdd_dept).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_orders = sc.parallelize([('o1'), ('o2')])\n",
    "rdd_products =sc.parallelize([('facewash'), ('handwash'), ('bodywash')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('o1', 'facewash'),\n",
       " ('o1', 'handwash'),\n",
       " ('o1', 'bodywash'),\n",
       " ('o2', 'facewash'),\n",
       " ('o2', 'handwash'),\n",
       " ('o2', 'bodywash')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_orders.cartesian(rdd_products).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[80] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme.persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701548"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701548"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[80] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "### find the average pressure per sensor per day\n",
    "### removing the rows with columns less than 9\n",
    "### removing the rows with pressure value not defined\n",
    "### creating key value pair with key as sensor_id and timestamp(date part), and value as pressure and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bme_mapped = rdd_bme_split.filter(lambda row : len(row) == 9)\\\n",
    "                              .filter(lambda row : row[6] != '')\\\n",
    "                              .map(lambda row : ((row[1], row[5][0:10]), (float(row[6]), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2266', '2017-07-01'), (95270.27, 1)),\n",
       " (('2292', '2017-07-01'), (94355.83, 1)),\n",
       " (('3096', '2017-07-01'), (95155.81, 1)),\n",
       " (('3428', '2017-07-01'), (94679.57, 1)),\n",
       " (('3472', '2017-07-01'), (94327.88, 1)),\n",
       " (('1952', '2017-07-01'), (95314.52, 1)),\n",
       " (('1846', '2017-07-01'), (93616.77, 1)),\n",
       " (('3512', '2017-07-01'), (94962.39, 1)),\n",
       " (('2228', '2017-07-01'), (94982.91, 1)),\n",
       " (('3438', '2017-07-01'), (95099.81, 1))]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_mapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2292', '2017-07-01'), (54339274.579999946, 577)),\n",
       " (('1952', '2017-07-01'), (51929139.60000006, 546)),\n",
       " (('3512', '2017-07-01'), (54215394.09000003, 572)),\n",
       " (('3438', '2017-07-01'), (53622333.59000005, 565)),\n",
       " (('3474', '2017-07-01'), (54400704.23999998, 576))]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_mapped.reduceByKey(lambda x,y : (x[0]+y[0], x[1]+y[1])).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2292', '2017-07-01'), 94175.51920277288),\n",
       " (('1952', '2017-07-01'), 95108.3142857144),\n",
       " (('3512', '2017-07-01'), 94782.15750000006),\n",
       " (('3438', '2017-07-01'), 94906.78511504433),\n",
       " (('3474', '2017-07-01'), 94445.6670833333),\n",
       " (('2232', '2017-07-01'), 94495.66624548737),\n",
       " (('3738', '2017-07-01'), 95039.42874564463),\n",
       " (('2040', '2017-07-01'), 94750.76236614858),\n",
       " (('3432', '2017-07-01'), 95144.56160558463),\n",
       " (('2294', '2017-07-01'), 92483.59339754815)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_mapped.reduceByKey(lambda x,y : (x[0]+y[0], x[1]+y[1]))\\\n",
    "              .map(lambda x : (x[0], x[1][0]/ x[1][1])).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregateByKey\n",
    "### find the max temperature for every date\n",
    "### seqOp is defined which finds the max value in each partition for each key\n",
    "### combOp is defined which finds the max value across partition for each key\n",
    "### zeroVal is defined to store the initial value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bme_mapped_temp = rdd_bme_split.filter(lambda row : len(row) == 9)\\\n",
    "                      .filter(lambda row : row[7] != '')\\\n",
    "                      .map(lambda row : ((row[1], row[5][0:10]), float(row[7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroVal = 0\n",
    "\n",
    "def seqOp(acc, element):\n",
    "    if acc > element :\n",
    "        return acc\n",
    "    else : \n",
    "        return element\n",
    "\n",
    "def combOp(acc1, acc2):\n",
    "    if acc1 > acc2:\n",
    "        return acc1\n",
    "    else:\n",
    "        return acc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2292', '2017-07-01'), 43.38),\n",
       " (('1952', '2017-07-01'), 45.6),\n",
       " (('3512', '2017-07-01'), 52.07),\n",
       " (('3438', '2017-07-01'), 48.96),\n",
       " (('3474', '2017-07-01'), 41.09)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_mapped_temp.aggregateByKey(zeroVal, seqOp, combOp).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combineByKey\n",
    "### find the min humidity for every date\n",
    "### create combiner is similar to zeroVal in aggregateByKey. It initializes value for each key\n",
    "### mergeValue does the local aggregations\n",
    "### mergeCombiner does the aggregations for the keys across partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bme_mapped_hum = rdd_bme_split.filter(lambda row : len(row) == 9)\\\n",
    "                     .filter(lambda row : row[8] != '')\\\n",
    "                     .map(lambda row : ((row[1], row[5][0:10]), float(row[8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCombiner(humidity):\n",
    "    return humidity\n",
    "\n",
    "def mergeValue(acc, element):\n",
    "    if acc < element:\n",
    "        return acc\n",
    "    else:\n",
    "        return element\n",
    "\n",
    "def mergeCombiner(acc1, acc2):\n",
    "    if acc1 < acc2:\n",
    "        return acc1\n",
    "    else:\n",
    "        return acc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2292', '2017-07-01'), 14.14),\n",
       " (('1952', '2017-07-01'), 15.93),\n",
       " (('3512', '2017-07-01'), 8.85),\n",
       " (('3438', '2017-07-01'), 8.76),\n",
       " (('3474', '2017-07-01'), 15.14),\n",
       " (('2232', '2017-07-01'), 14.9),\n",
       " (('3738', '2017-07-01'), 3.59),\n",
       " (('2040', '2017-07-01'), 14.8),\n",
       " (('3432', '2017-07-01'), 10.09),\n",
       " (('2294', '2017-07-01'), 11.5)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bme_mapped_hum.combineByKey(createCombiner, mergeValue, mergeCombiner).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = sc.broadcast([1, 2, 3, 4, 5])\n",
    "b.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([0, 0]).flatMap(lambda x: b.value).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5], [6, 7]]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "sorted(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 5, 6, 7], [2, 3]]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdd.repartition(2).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdd.repartition(10).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bme2 = rdd_bme.repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-177-936e39101451>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd_bme2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\mohit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \"\"\"\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32mC:\\spark\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1038\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m                 \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mohit\\appdata\\local\\programs\\python\\python37\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rdd_bme2.glom().collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
